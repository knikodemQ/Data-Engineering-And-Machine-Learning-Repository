{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd1bedc",
   "metadata": {},
   "source": [
    "# Project 01: Pandas Basics - Data Analysis and Manipulation\n",
    "\n",
    "## Dataset\n",
    "The project uses `proj1_ex01.csv` which contains sample data for demonstrating pandas functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9913652",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data')\n",
    "OUTPUT_PATH = Path('output')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13844c8c",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7, 8)\n",
      "Columns: ['First column', 'two', 'three', 'SOME;NAME', 'five', 'What is this even?', 'seven', 'eight']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH / \"proj1_ex01.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c36d0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "First column",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "two",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "three",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SOME;NAME",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "five",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "What is this even?",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seven",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "eight",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e48342f1-0ac1-40ac-b4e8-a1787a22ec7e",
       "rows": [
        [
         "0",
         "0.348553924470116",
         "-0.14509562920877161",
         "-0.012336991474672475",
         "9",
         "red",
         "good",
         "quarrelsome",
         "2016-05-26 09:33:42"
        ],
        [
         "1",
         "-1.4938530178231937",
         "0.12436946488785079",
         "1.4611100361038865",
         "4",
         "red",
         "bad",
         "doctor",
         "2016-12-03 18:55:52"
        ],
        [
         "2",
         "-0.3258910368497604",
         null,
         "-0.42191202598625566",
         "2",
         "red",
         "average",
         "large",
         "2016-05-15 11:49:26"
        ],
        [
         "3",
         "-0.5065957165861508",
         "0.3991147675939107",
         "-0.26502607502330217 ",
         "5",
         "green",
         "average",
         "muddled",
         "2015-01-30 22:33:29"
        ],
        [
         "4",
         null,
         "-0.6913144223047157",
         "-0.26502607502330217",
         "2",
         "blue",
         "good",
         "coordinated",
         "2015-11-20 00:15:35"
        ],
        [
         "5",
         "0.5271122588523375",
         "2.584347847701393",
         "-0.26502607502330217XYZ",
         "2",
         "blue",
         "good",
         "separate",
         "2017-11-17 09:58:54"
        ],
        [
         "6",
         "-1.55529041326908",
         "unknown",
         "-0.7732649697439955",
         "5",
         "green",
         "bad",
         "bright",
         "2017-05-01 10:32:41"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First column</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "      <th>SOME;NAME</th>\n",
       "      <th>five</th>\n",
       "      <th>What is this even?</th>\n",
       "      <th>seven</th>\n",
       "      <th>eight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.348554</td>\n",
       "      <td>-0.14509562920877161</td>\n",
       "      <td>-0.012336991474672475</td>\n",
       "      <td>9</td>\n",
       "      <td>red</td>\n",
       "      <td>good</td>\n",
       "      <td>quarrelsome</td>\n",
       "      <td>2016-05-26 09:33:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.493853</td>\n",
       "      <td>0.12436946488785079</td>\n",
       "      <td>1.4611100361038865</td>\n",
       "      <td>4</td>\n",
       "      <td>red</td>\n",
       "      <td>bad</td>\n",
       "      <td>doctor</td>\n",
       "      <td>2016-12-03 18:55:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.325891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.42191202598625566</td>\n",
       "      <td>2</td>\n",
       "      <td>red</td>\n",
       "      <td>average</td>\n",
       "      <td>large</td>\n",
       "      <td>2016-05-15 11:49:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.506596</td>\n",
       "      <td>0.3991147675939107</td>\n",
       "      <td>-0.26502607502330217</td>\n",
       "      <td>5</td>\n",
       "      <td>green</td>\n",
       "      <td>average</td>\n",
       "      <td>muddled</td>\n",
       "      <td>2015-01-30 22:33:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.6913144223047157</td>\n",
       "      <td>-0.26502607502330217</td>\n",
       "      <td>2</td>\n",
       "      <td>blue</td>\n",
       "      <td>good</td>\n",
       "      <td>coordinated</td>\n",
       "      <td>2015-11-20 00:15:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.527112</td>\n",
       "      <td>2.584347847701393</td>\n",
       "      <td>-0.26502607502330217XYZ</td>\n",
       "      <td>2</td>\n",
       "      <td>blue</td>\n",
       "      <td>good</td>\n",
       "      <td>separate</td>\n",
       "      <td>2017-11-17 09:58:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.555290</td>\n",
       "      <td>unknown</td>\n",
       "      <td>-0.7732649697439955</td>\n",
       "      <td>5</td>\n",
       "      <td>green</td>\n",
       "      <td>bad</td>\n",
       "      <td>bright</td>\n",
       "      <td>2017-05-01 10:32:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   First column                   two                    three  SOME;NAME  \\\n",
       "0      0.348554  -0.14509562920877161    -0.012336991474672475          9   \n",
       "1     -1.493853   0.12436946488785079       1.4611100361038865          4   \n",
       "2     -0.325891                   NaN     -0.42191202598625566          2   \n",
       "3     -0.506596    0.3991147675939107    -0.26502607502330217           5   \n",
       "4           NaN   -0.6913144223047157     -0.26502607502330217          2   \n",
       "5      0.527112     2.584347847701393  -0.26502607502330217XYZ          2   \n",
       "6     -1.555290               unknown      -0.7732649697439955          5   \n",
       "\n",
       "    five What is this even?        seven                eight  \n",
       "0    red               good  quarrelsome  2016-05-26 09:33:42  \n",
       "1    red                bad       doctor  2016-12-03 18:55:52  \n",
       "2    red            average        large  2016-05-15 11:49:26  \n",
       "3  green            average      muddled  2015-01-30 22:33:29  \n",
       "4   blue               good  coordinated  2015-11-20 00:15:35  \n",
       "5   blue               good     separate  2017-11-17 09:58:54  \n",
       "6  green                bad       bright  2017-05-01 10:32:41  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc14b4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   First column        6 non-null      float64\n",
      " 1   two                 6 non-null      object \n",
      " 2   three               7 non-null      object \n",
      " 3   SOME;NAME           7 non-null      int64  \n",
      " 4   five                7 non-null      object \n",
      " 5   What is this even?  7 non-null      object \n",
      " 6   seven               7 non-null      object \n",
      " 7   eight               7 non-null      object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 576.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Basic dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbedf6",
   "metadata": {},
   "source": [
    "## 3. Column Analysis and Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fe7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: First column\n",
      "  Type: float\n",
      "  Missing: 14.29%\n",
      "  Unique values: 6\n",
      "\n",
      "Column: two\n",
      "  Type: object\n",
      "  Missing: 14.29%\n",
      "  Unique values: 6\n",
      "\n",
      "Column: three\n",
      "  Type: object\n",
      "  Missing: 0.0%\n",
      "  Unique values: 7\n",
      "\n",
      "Column: SOME;NAME\n",
      "  Type: integer\n",
      "  Missing: 0.0%\n",
      "  Unique values: 4\n",
      "\n",
      "Column: five\n",
      "  Type: object\n",
      "  Missing: 0.0%\n",
      "  Unique values: 3\n",
      "\n",
      "Column: What is this even?\n",
      "  Type: object\n",
      "  Missing: 0.0%\n",
      "  Unique values: 3\n",
      "\n",
      "Column: seven\n",
      "  Type: object\n",
      "  Missing: 0.0%\n",
      "  Unique values: 7\n",
      "\n",
      "Column: eight\n",
      "  Type: object\n",
      "  Missing: 0.0%\n",
      "  Unique values: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_columns(dataframe):\n",
    "    columns_info = []\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        missing_percent = dataframe[col].isnull().mean()\n",
    "        \n",
    "        # Determine data type\n",
    "        if pd.api.types.is_integer_dtype(dataframe[col]):\n",
    "            data_type = \"integer\"\n",
    "        elif pd.api.types.is_float_dtype(dataframe[col]):\n",
    "            data_type = \"float\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dataframe[col]):\n",
    "            data_type = \"datetime\"\n",
    "        elif pd.api.types.is_bool_dtype(dataframe[col]):\n",
    "            data_type = \"boolean\"\n",
    "        else:\n",
    "            data_type = \"object\"\n",
    "        \n",
    "        column_info = {\n",
    "            \"name\": col,\n",
    "            \"missing_percent\": round(missing_percent * 100, 2),\n",
    "            \"data_type\": data_type,\n",
    "            \"unique_values\": dataframe[col].nunique(),\n",
    "            \"memory_usage\": dataframe[col].memory_usage(deep=True)\n",
    "        }\n",
    "        columns_info.append(column_info)\n",
    "    \n",
    "    return columns_info\n",
    "\n",
    "# Analyze columns\n",
    "columns_metadata = analyze_columns(df)\n",
    "\n",
    "# Display the analysis\n",
    "for info in columns_metadata:\n",
    "    print(f\"Column: {info['name']}\")\n",
    "    print(f\"  Type: {info['data_type']}\")\n",
    "    print(f\"  Missing: {info['missing_percent']}%\")\n",
    "    print(f\"  Unique values: {info['unique_values']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e1a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column metadata saved to output/column_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# Save column metadata to JSON\n",
    "with open(OUTPUT_PATH / \"column_analysis.json\", \"w\") as json_file:\n",
    "    json.dump(columns_metadata, json_file, indent=2)\n",
    "\n",
    "print(\"Column metadata saved to output/column_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af3c6f",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First column (numerical)\n",
      "  count: 6\n",
      "  mean: -0.5009940002009552\n",
      "  std: 0.8839385203395562\n",
      "  min: -1.55529041326908\n",
      "  25%: -1.247038692513933\n",
      "  50%: -0.4162433767179556\n",
      "  75%: 0.1799426841401469\n",
      "  max: 0.5271122588523375\n",
      "  missing: 1\n",
      "\n",
      "two (categorical)\n",
      "  count: 6\n",
      "  unique: 6\n",
      "  most_frequent: -0.14509562920877161\n",
      "  frequency: 1\n",
      "  missing: 1\n",
      "\n",
      "three (categorical)\n",
      "  count: 7\n",
      "  unique: 7\n",
      "  most_frequent: -0.012336991474672475\n",
      "  frequency: 1\n",
      "  missing: 0\n"
     ]
    }
   ],
   "source": [
    "def generate_statistical_summary(dataframe):\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "            # Numerical column statistics\n",
    "            stats[col] = {\n",
    "                \"type\": \"numerical\",\n",
    "                \"count\": int(dataframe[col].count()),\n",
    "                \"mean\": float(dataframe[col].mean()),\n",
    "                \"std\": float(dataframe[col].std()),\n",
    "                \"min\": float(dataframe[col].min()),\n",
    "                \"25%\": float(dataframe[col].quantile(0.25)),\n",
    "                \"50%\": float(dataframe[col].quantile(0.50)),\n",
    "                \"75%\": float(dataframe[col].quantile(0.75)),\n",
    "                \"max\": float(dataframe[col].max()),\n",
    "                \"missing\": int(dataframe[col].isnull().sum())\n",
    "            }\n",
    "        else:\n",
    "            # Categorical column statistics\n",
    "            value_counts = dataframe[col].value_counts()\n",
    "            most_frequent = value_counts.index[0] if len(value_counts) > 0 else None\n",
    "            \n",
    "            stats[col] = {\n",
    "                \"type\": \"categorical\",\n",
    "                \"count\": int(dataframe[col].count()),\n",
    "                \"unique\": int(dataframe[col].nunique()),\n",
    "                \"most_frequent\": str(most_frequent) if most_frequent is not None else None,\n",
    "                \"frequency\": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,\n",
    "                \"missing\": int(dataframe[col].isnull().sum())\n",
    "            }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Generate statistical summary\n",
    "statistical_summary = generate_statistical_summary(df)\n",
    "\n",
    "# Display summary for first few columns\n",
    "for col_name, stats in list(statistical_summary.items())[:3]:\n",
    "    print(f\"\\n{col_name} ({stats['type']})\")\n",
    "    for key, value in stats.items():\n",
    "        if key != 'type':\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14035bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical summary saved to output/statistical_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Save statistical summary to JSON\n",
    "with open(OUTPUT_PATH / \"statistical_summary.json\", \"w\") as f:\n",
    "    json.dump(statistical_summary, f, indent=2)\n",
    "\n",
    "print(\"Statistical summary saved to output/statistical_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15a4e6",
   "metadata": {},
   "source": [
    "## 5. Column Name Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6611c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['First column', 'two', 'three', 'SOME;NAME', 'five', 'What is this even?', 'seven', 'eight']\n",
      "Cleaned columns: ['first_column', 'two', 'three', 'somename', 'five', 'what_is_this_even', 'seven', 'eight']\n"
     ]
    }
   ],
   "source": [
    "def clean_column_names(dataframe):\n",
    "    \n",
    "    df_cleaned = dataframe.copy()\n",
    "    \n",
    "    # Clean column names: remove special characters, convert to lowercase, replace spaces with underscores\n",
    "    cleaned_columns = []\n",
    "    for col in df_cleaned.columns:\n",
    "        # Remove special characters except letters, numbers, spaces, and underscores\n",
    "        cleaned = re.sub(r\"[^A-Za-z0-9_ ]\", \"\", str(col))\n",
    "        # Convert to lowercase and replace spaces with underscores\n",
    "        cleaned = cleaned.lower().replace(\" \", \"_\").strip()\n",
    "        # Remove multiple consecutive underscores\n",
    "        cleaned = re.sub(r\"_+\", \"_\", cleaned)\n",
    "        # Remove leading/trailing underscores\n",
    "        cleaned = cleaned.strip(\"_\")\n",
    "        cleaned_columns.append(cleaned)\n",
    "    \n",
    "    df_cleaned.columns = cleaned_columns\n",
    "    return df_cleaned\n",
    "\n",
    "# Show original column names\n",
    "print(\"Original columns:\", list(df.columns))\n",
    "\n",
    "# Clean column names\n",
    "df_clean_cols = clean_column_names(df)\n",
    "\n",
    "# Show cleaned column names\n",
    "print(\"Cleaned columns:\", list(df_clean_cols.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ea5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with cleaned columns saved to output/cleaned_columns_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Save dataset with cleaned column names\n",
    "df_clean_cols.to_csv(OUTPUT_PATH / \"cleaned_columns_dataset.csv\", index=False)\n",
    "print(\"Dataset with cleaned columns saved to output/cleaned_columns_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bde21",
   "metadata": {},
   "source": [
    "## 6. Data Export in Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ee16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported to JSON: output\\pandas_basics_dataset.json\n",
      "✓ Exported to PICKLE: output\\pandas_basics_dataset.pickle\n",
      "✗ Failed to export to EXCEL: No module named 'openpyxl'\n",
      "✗ Failed to export to PARQUET: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n"
     ]
    }
   ],
   "source": [
    "# Export to various formats\n",
    "export_formats = {\n",
    "    'json': lambda df, path: df.to_json(path, orient='records', indent=2),\n",
    "    'pickle': lambda df, path: df.to_pickle(path),\n",
    "    'excel': lambda df, path: df.to_excel(path, index=False),\n",
    "    'parquet': lambda df, path: df.to_parquet(path, index=False)\n",
    "}\n",
    "\n",
    "base_filename = \"pandas_basics_dataset\"\n",
    "\n",
    "for format_name, export_func in export_formats.items():\n",
    "    try:\n",
    "        file_path = OUTPUT_PATH / f\"{base_filename}.{format_name}\"\n",
    "        export_func(df_clean_cols, file_path)\n",
    "        print(f\"✓ Exported to {format_name.upper()}: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to export to {format_name.upper()}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e85591",
   "metadata": {},
   "source": [
    "## 7. Working with Additional Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fb49ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing proj1_ex05.pkl:\n",
      "  Loaded pickle file with shape: (7, 3)\n",
      "  Selected columns 1-2, shape: (7, 2)\n",
      "  Filtered rows starting with 'v': 2\n",
      "  Saved processed data to: output\\processed_proj1_ex05.csv\n",
      "\n",
      "Processing proj1_ex06.json:\n",
      "  Normalized JSON to DataFrame with shape: (6, 8)\n",
      "  Saved normalized data to: output\\normalized_proj1_ex06.csv\n"
     ]
    }
   ],
   "source": [
    "# Load and process additional datasets if they exist\n",
    "additional_files = ['proj1_ex05.pkl', 'proj1_ex06.json']\n",
    "\n",
    "for filename in additional_files:\n",
    "    file_path = DATA_PATH / filename\n",
    "    \n",
    "    if file_path.exists():\n",
    "        print(f\"\\nProcessing {filename}:\")\n",
    "        \n",
    "        try:\n",
    "            if filename.endswith('.pkl'):\n",
    "                data = pd.read_pickle(file_path)\n",
    "                print(f\"  Loaded pickle file with shape: {data.shape}\")\n",
    "                \n",
    "                # Demonstrate data selection and processing\n",
    "                if hasattr(data, 'iloc') and data.shape[1] >= 3:\n",
    "                    selected_columns = data.iloc[:, 1:3]\n",
    "                    print(f\"  Selected columns 1-2, shape: {selected_columns.shape}\")\n",
    "                    \n",
    "                    # Filter rows based on index pattern if applicable\n",
    "                    if hasattr(data.index, 'str'):\n",
    "                        filtered_rows = data[data.index.str.startswith('v', na=False)]\n",
    "                        if not filtered_rows.empty:\n",
    "                            print(f\"  Filtered rows starting with 'v': {len(filtered_rows)}\")\n",
    "                            \n",
    "                            # Process and save\n",
    "                            result_df = selected_columns.loc[filtered_rows.index].fillna('')\n",
    "                            output_file = OUTPUT_PATH / f\"processed_{filename.replace('.pkl', '.csv')}\"\n",
    "                            result_df.to_csv(output_file)\n",
    "                            print(f\"  Saved processed data to: {output_file}\")\n",
    "            \n",
    "            elif filename.endswith('.json'):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                # Normalize JSON data\n",
    "                normalized_df = pd.json_normalize(json_data)\n",
    "                print(f\"  Normalized JSON to DataFrame with shape: {normalized_df.shape}\")\n",
    "                \n",
    "                # Save normalized data\n",
    "                output_file = OUTPUT_PATH / f\"normalized_{filename.replace('.json', '.csv')}\"\n",
    "                normalized_df.to_csv(output_file, index=False)\n",
    "                print(f\"  Saved normalized data to: {output_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n{filename} not found, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1154ea",
   "metadata": {},
   "source": [
    "## 8. Summary and Results\n",
    "\n",
    "This notebook demonstrated essential pandas operations:\n",
    "\n",
    "1. **Data Loading**: Successfully loaded CSV data and explored its structure\n",
    "2. **Column Analysis**: Generated comprehensive metadata for each column\n",
    "3. **Statistical Summaries**: Created detailed statistics for both numerical and categorical columns\n",
    "4. **Data Cleaning**: Normalized column names using regex patterns\n",
    "5. **Multi-format Export**: Saved data in JSON, Pickle, Excel, and Parquet formats\n",
    "6. **Advanced Processing**: Handled additional data files with filtering and normalization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
