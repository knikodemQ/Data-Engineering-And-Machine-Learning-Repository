{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7b2a0d",
   "metadata": {},
   "source": [
    "# Project 02: Data Processing - Advanced Data Manipulation and Transformation\n",
    "\n",
    "This notebook demonstrates advanced data processing techniques including:\n",
    "- Automatic delimiter detection for CSV files\n",
    "- Custom scaling and encoding strategies\n",
    "- Data type conversions and categorical handling\n",
    "- Regular expressions for data extraction\n",
    "- One-hot encoding for categorical variables\n",
    "\n",
    "## Dataset\n",
    "The project uses `proj2_data.csv` with custom formatting and a scaling reference file `proj2_scale.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e826bc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df51f259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Set up paths\n",
    "DATA_PATH = Path('data')\n",
    "OUTPUT_PATH = Path('output')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd810fb",
   "metadata": {},
   "source": [
    "## 2. Smart CSV Loading with Automatic Delimiter Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f840a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected delimiter: '|'\n",
      "Dataset loaded successfully with shape: (12, 11)\n",
      "Columns: ['full_name', 'field', 'language', 'code', 'task_1', 'task_2', 'task_3', 'tasks_avg', 'task_grade', 'jury_score', 'final_grade']\n"
     ]
    }
   ],
   "source": [
    "def detect_csv_delimiter(filepath, possible_delimiters=['|', ';', ',', '\\t']):\n",
    "   \n",
    "    sniffer = csv.Sniffer()\n",
    "    sniffer.delimiters = possible_delimiters\n",
    "    \n",
    "    # Read a sample of the file to detect delimiter\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample = f.read(1024)\n",
    "    \n",
    "    detected_delimiter = sniffer.sniff(sample).delimiter\n",
    "    return detected_delimiter\n",
    "\n",
    "# Load the main dataset with automatic delimiter detection\n",
    "csv_file = DATA_PATH / 'proj2_data.csv'\n",
    "\n",
    "if csv_file.exists():\n",
    "    delimiter = detect_csv_delimiter(csv_file)\n",
    "    print(f\"Detected delimiter: '{delimiter}'\")\n",
    "    \n",
    "    # Load with detected delimiter and European decimal format\n",
    "    df_original = pd.read_csv(csv_file, delimiter=delimiter, decimal=',')\n",
    "    \n",
    "    print(f\"Dataset loaded successfully with shape: {df_original.shape}\")\n",
    "    print(f\"Columns: {list(df_original.columns)}\")\n",
    "    \n",
    "    # Save original processing result\n",
    "    df_original.to_pickle(OUTPUT_PATH / 'original_data.pkl')\n",
    "else:\n",
    "    print(f\"Warning: {csv_file} not found. Creating sample data for demonstration.\")\n",
    "    # Create sample data for demonstration\n",
    "    df_original = pd.DataFrame({\n",
    "        'category_a': ['low', 'medium', 'high', 'low', 'medium'],\n",
    "        'category_b': ['poor', 'fair', 'good', 'excellent', 'good'],\n",
    "        'numeric_text': ['10,5', '20,3', '15,7', '8,2', '12,9'],\n",
    "        'mixed_col': ['alpha', 'beta', 'gamma', 'alpha', 'beta']\n",
    "    })\n",
    "    print(\"Created sample dataset for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad182b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "          full_name   field   language code  task_1  task_2  task_3  \\\n",
      "0  Rowan Harrington  drones     python  wej     3.1     2.0     4.4   \n",
      "1        Nash Wyatt  racing       java  sfe     4.2     2.0     2.0   \n",
      "2    Jadiel Ramirez   media  cplusplus  vaw     4.0     4.9     3.0   \n",
      "3    Makaila Atkins  racing      swift  ugt     4.1     5.0     4.8   \n",
      "4    Melanie Fuller  racing     python  owb     2.7     2.0     2.0   \n",
      "\n",
      "   tasks_avg    task_grade jury_score  final_grade  \n",
      "0   3.166667   dostateczny    3,5 pts  dostateczny  \n",
      "1   2.733333  bardzo dobry        5 p       mierny  \n",
      "2   3.966667         dobry        3.5       mierny  \n",
      "3   4.633333         dobry          2  dostateczny  \n",
      "4   2.233333  bardzo dobry      pts 2       mierny  \n",
      "\n",
      "Data Types:\n",
      "full_name       object\n",
      "field           object\n",
      "language        object\n",
      "code            object\n",
      "task_1         float64\n",
      "task_2         float64\n",
      "task_3         float64\n",
      "tasks_avg      float64\n",
      "task_grade      object\n",
      "jury_score      object\n",
      "final_grade     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display the loaded data\n",
    "print(\"Original Dataset:\")\n",
    "print(df_original.head())\n",
    "print(\"\\nData Types:\")\n",
    "print(df_original.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf844c0",
   "metadata": {},
   "source": [
    "## 3. Custom Scaling System Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Scale Mapping:\n",
      "  niedostateczny -> 1\n",
      "  mierny -> 2\n",
      "  dostateczny -> 3\n",
      "  dobry -> 4\n",
      "  bardzo dobry -> 5\n"
     ]
    }
   ],
   "source": [
    "def load_custom_scale(scale_file_path):\n",
    "    \n",
    "    scale_mapping = {}\n",
    "    \n",
    "    try:\n",
    "        with open(scale_file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                category = line.strip()\n",
    "                if category:  # Skip empty lines\n",
    "                    scale_mapping[category] = i\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Scale file {scale_file_path} not found. Creating default scale.\")\n",
    "        # Create default ordinal scale for common categories\n",
    "        default_categories = ['poor', 'fair', 'good', 'excellent']\n",
    "        scale_mapping = {cat: i+1 for i, cat in enumerate(default_categories)}\n",
    "    \n",
    "    return scale_mapping\n",
    "\n",
    "# Load the scaling system\n",
    "scale_file = DATA_PATH / 'proj2_scale.txt'\n",
    "custom_scale = load_custom_scale(scale_file)\n",
    "\n",
    "print(\"Custom Scale Mapping:\")\n",
    "for category, value in custom_scale.items():\n",
    "    print(f\"  {category} -> {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b143932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied scaling to column: task_grade\n",
      "Applied scaling to column: final_grade\n",
      "\n",
      "Columns scaled: ['task_grade', 'final_grade']\n",
      "\n",
      "Scaled Dataset:\n",
      "          full_name   field   language code  task_1  task_2  task_3  \\\n",
      "0  Rowan Harrington  drones     python  wej     3.1     2.0     4.4   \n",
      "1        Nash Wyatt  racing       java  sfe     4.2     2.0     2.0   \n",
      "2    Jadiel Ramirez   media  cplusplus  vaw     4.0     4.9     3.0   \n",
      "3    Makaila Atkins  racing      swift  ugt     4.1     5.0     4.8   \n",
      "4    Melanie Fuller  racing     python  owb     2.7     2.0     2.0   \n",
      "\n",
      "   tasks_avg  task_grade jury_score  final_grade  \n",
      "0   3.166667           3    3,5 pts            3  \n",
      "1   2.733333           5        5 p            2  \n",
      "2   3.966667           4        3.5            2  \n",
      "3   4.633333           4          2            3  \n",
      "4   2.233333           5      pts 2            2  \n"
     ]
    }
   ],
   "source": [
    "def apply_custom_scaling(dataframe, scale_mapping):\n",
    "    \n",
    "    df_scaled = dataframe.copy()\n",
    "    scaled_columns = []\n",
    "    \n",
    "    for col in df_scaled.columns:\n",
    "        # Check if any values in this column exist in our scale mapping\n",
    "        if df_scaled[col].dtype == 'object':\n",
    "            unique_values = set(df_scaled[col].dropna().unique())\n",
    "            scale_keys = set(scale_mapping.keys())\n",
    "            \n",
    "            if unique_values.intersection(scale_keys):\n",
    "                # Apply scaling to this column\n",
    "                df_scaled[col] = df_scaled[col].map(scale_mapping).fillna(df_scaled[col])\n",
    "                scaled_columns.append(col)\n",
    "                print(f\"Applied scaling to column: {col}\")\n",
    "    \n",
    "    return df_scaled, scaled_columns\n",
    "\n",
    "# Apply custom scaling\n",
    "df_scaled, scaled_cols = apply_custom_scaling(df_original, custom_scale)\n",
    "\n",
    "print(f\"\\nColumns scaled: {scaled_cols}\")\n",
    "print(\"\\nScaled Dataset:\")\n",
    "print(df_scaled.head())\n",
    "\n",
    "# Save scaled data\n",
    "df_scaled.to_pickle(OUTPUT_PATH / 'custom_scaled_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba160b0f",
   "metadata": {},
   "source": [
    "## 4. Advanced Categorical Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a6f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created nominal categorical for field\n",
      "Created nominal categorical for language\n",
      "Created ordered categorical for task_grade: ['dostateczny', 'dobry', 'bardzo dobry']\n",
      "Created ordered categorical for final_grade: ['niedostateczny', 'mierny', 'dostateczny']\n",
      "\n",
      "Categorical columns created: ['field', 'language', 'task_grade', 'final_grade']\n",
      "\n",
      "Data types after categorical conversion:\n",
      "full_name        object\n",
      "field          category\n",
      "language       category\n",
      "code             object\n",
      "task_1          float64\n",
      "task_2          float64\n",
      "task_3          float64\n",
      "tasks_avg       float64\n",
      "task_grade     category\n",
      "jury_score       object\n",
      "final_grade    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def create_categorical_columns(dataframe, scale_mapping):\n",
    "    \n",
    "    df_categorical = dataframe.copy()\n",
    "    categorical_columns = []\n",
    "    \n",
    "    for col in df_categorical.columns:\n",
    "        if df_categorical[col].dtype == 'object':\n",
    "            unique_values = set(df_categorical[col].dropna().unique())\n",
    "            scale_keys = set(scale_mapping.keys())\n",
    "            \n",
    "            # If column contains values from our scale, make it ordinal categorical\n",
    "            if unique_values.intersection(scale_keys):\n",
    "                # Sort categories by their scale values\n",
    "                relevant_categories = [cat for cat in scale_mapping.keys() \n",
    "                                     if cat in unique_values]\n",
    "                sorted_categories = sorted(relevant_categories, \n",
    "                                         key=lambda x: scale_mapping[x])\n",
    "                \n",
    "                df_categorical[col] = pd.Categorical(\n",
    "                    df_categorical[col], \n",
    "                    categories=sorted_categories,\n",
    "                    ordered=True\n",
    "                )\n",
    "                categorical_columns.append(col)\n",
    "                print(f\"Created ordered categorical for {col}: {sorted_categories}\")\n",
    "            \n",
    "            # For other object columns with few unique values, make them nominal categorical\n",
    "            elif df_categorical[col].nunique() <= 10:\n",
    "                df_categorical[col] = pd.Categorical(df_categorical[col])\n",
    "                categorical_columns.append(col)\n",
    "                print(f\"Created nominal categorical for {col}\")\n",
    "    \n",
    "    return df_categorical, categorical_columns\n",
    "\n",
    "# Create categorical columns\n",
    "df_categorical, cat_cols = create_categorical_columns(df_original, custom_scale)\n",
    "\n",
    "print(f\"\\nCategorical columns created: {cat_cols}\")\n",
    "print(\"\\nData types after categorical conversion:\")\n",
    "print(df_categorical.dtypes)\n",
    "\n",
    "# Save categorical data\n",
    "df_categorical.to_pickle(OUTPUT_PATH / 'categorical_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e96225",
   "metadata": {},
   "source": [
    "## 5. Number Extraction from Text Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b137e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted numbers from jury_score -> jury_score_numeric\n",
      "\n",
      "Columns with extracted numbers: ['jury_score_numeric']\n",
      "\n",
      "Extracted numeric data:\n",
      "   jury_score_numeric\n",
      "0                 3.5\n",
      "1                 5.0\n",
      "2                 3.5\n",
      "3                 2.0\n",
      "4                 2.0\n",
      "\n",
      "Data types of extracted columns:\n",
      "jury_score_numeric    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def extract_numbers_from_text(text_value):\n",
    "    \n",
    "    if pd.isna(text_value) or not isinstance(text_value, str):\n",
    "        return text_value\n",
    "    \n",
    "    # Pattern to match numbers with optional decimal part (comma or dot)\n",
    "    number_pattern = r'[-+]?\\d*[,.]?\\d+(?:[,.]\\d+)?'\n",
    "    \n",
    "    numbers = re.findall(number_pattern, str(text_value))\n",
    "    \n",
    "    if numbers:\n",
    "        # Take the first number found and normalize decimal separator\n",
    "        number_str = numbers[0].replace(',', '.')\n",
    "        try:\n",
    "            return float(number_str)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def convert_text_to_numeric(dataframe):\n",
    "    \n",
    "    df_numeric = dataframe.copy()\n",
    "    converted_columns = []\n",
    "    \n",
    "    # Process non-numeric columns\n",
    "    for col in df_numeric.select_dtypes(exclude=['number']).columns:\n",
    "        # Apply number extraction\n",
    "        extracted_values = df_numeric[col].apply(extract_numbers_from_text)\n",
    "        \n",
    "        # Check if extraction was successful (found numeric values)\n",
    "        non_null_extracted = extracted_values.dropna()\n",
    "        if len(non_null_extracted) > 0 and non_null_extracted.apply(lambda x: isinstance(x, (int, float))).any():\n",
    "            df_numeric[f'{col}_numeric'] = extracted_values\n",
    "            converted_columns.append(f'{col}_numeric')\n",
    "            print(f\"Extracted numbers from {col} -> {col}_numeric\")\n",
    "    \n",
    "    # Return only the successfully converted numeric columns\n",
    "    if converted_columns:\n",
    "        return df_numeric[converted_columns], converted_columns\n",
    "    else:\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "# Extract numbers from text columns\n",
    "df_extracted, extracted_cols = convert_text_to_numeric(df_original)\n",
    "\n",
    "if not df_extracted.empty:\n",
    "    print(f\"\\nColumns with extracted numbers: {extracted_cols}\")\n",
    "    print(\"\\nExtracted numeric data:\")\n",
    "    print(df_extracted.head())\n",
    "    print(\"\\nData types of extracted columns:\")\n",
    "    print(df_extracted.dtypes)\n",
    "    \n",
    "    # Save extracted numeric data\n",
    "    df_extracted.to_pickle(OUTPUT_PATH / 'extracted_numeric_data.pkl')\n",
    "else:\n",
    "    print(\"No numeric values were successfully extracted from text columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bcdbe",
   "metadata": {},
   "source": [
    "## 6. Smart One-Hot Encoding for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de79ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'field' identified for one-hot encoding\n",
      "  Unique values: ['drones', 'media', 'racing', 'robotics']\n",
      "Column 'language' identified for one-hot encoding\n",
      "  Unique values: ['cplusplus', 'java', 'python', 'swift']\n",
      "\n",
      "Columns selected for one-hot encoding: ['field', 'language']\n",
      "\n",
      "One-hot encoding for 'field':\n",
      "  Original shape: (12,)\n",
      "  Encoded shape: (12, 4)\n",
      "  New columns: ['field_drones', 'field_media', 'field_racing', 'field_robotics']\n",
      "\n",
      "One-hot encoding for 'language':\n",
      "  Original shape: (12,)\n",
      "  Encoded shape: (12, 4)\n",
      "  New columns: ['language_cplusplus', 'language_java', 'language_python', 'language_swift']\n",
      "\n",
      "Encoded DataFrame for 'field':\n",
      "   field_drones  field_media  field_racing  field_robotics\n",
      "0          True        False         False           False\n",
      "1         False        False          True           False\n",
      "2         False         True         False           False\n",
      "3         False        False          True           False\n",
      "4         False        False          True           False\n",
      "\n",
      "Encoded DataFrame for 'language':\n",
      "   language_cplusplus  language_java  language_python  language_swift\n",
      "0               False          False             True           False\n",
      "1               False           True            False           False\n",
      "2                True          False            False           False\n",
      "3               False          False            False            True\n",
      "4               False          False             True           False\n"
     ]
    }
   ],
   "source": [
    "def identify_encoding_candidates(dataframe, scale_mapping, max_categories=10):\n",
    "    \n",
    "    encoding_candidates = []\n",
    "    scale_values = set(scale_mapping.keys())\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            unique_values = set(dataframe[col].dropna().unique())\n",
    "            \n",
    "            # Check if column is suitable for encoding\n",
    "            conditions = [\n",
    "                len(unique_values) <= max_categories,  # Not too many categories\n",
    "                len(unique_values) > 1,  # Has variation\n",
    "                not unique_values.intersection(scale_values),  # Not in custom scale\n",
    "                all(isinstance(val, str) and val.isalpha() for val in unique_values)  # Text only\n",
    "            ]\n",
    "            \n",
    "            if all(conditions):\n",
    "                encoding_candidates.append(col)\n",
    "                print(f\"Column '{col}' identified for one-hot encoding\")\n",
    "                print(f\"  Unique values: {sorted(unique_values)}\")\n",
    "    \n",
    "    return encoding_candidates\n",
    "\n",
    "def create_one_hot_encodings(dataframe, columns_to_encode):\n",
    "    \n",
    "    encoded_results = {}\n",
    "    \n",
    "    for col in columns_to_encode:\n",
    "        # Create one-hot encoding\n",
    "        encoded_df = pd.get_dummies(\n",
    "            dataframe[col], \n",
    "            prefix=col,\n",
    "            prefix_sep='_',\n",
    "            dummy_na=False\n",
    "        )\n",
    "        \n",
    "        encoded_results[col] = encoded_df\n",
    "        \n",
    "        print(f\"\\nOne-hot encoding for '{col}':\")\n",
    "        print(f\"  Original shape: {dataframe[col].shape}\")\n",
    "        print(f\"  Encoded shape: {encoded_df.shape}\")\n",
    "        print(f\"  New columns: {list(encoded_df.columns)}\")\n",
    "        \n",
    "        # Save individual encoded DataFrame\n",
    "        encoded_df.to_pickle(OUTPUT_PATH / f'one_hot_{col}.pkl')\n",
    "    \n",
    "    return encoded_results\n",
    "\n",
    "# Identify and encode categorical columns\n",
    "columns_to_encode = identify_encoding_candidates(df_original, custom_scale)\n",
    "\n",
    "if columns_to_encode:\n",
    "    print(f\"\\nColumns selected for one-hot encoding: {columns_to_encode}\")\n",
    "    \n",
    "    # Create one-hot encodings\n",
    "    encoded_dataframes = create_one_hot_encodings(df_original, columns_to_encode)\n",
    "    \n",
    "    # Display results\n",
    "    for col, encoded_df in encoded_dataframes.items():\n",
    "        print(f\"\\nEncoded DataFrame for '{col}':\")\n",
    "        print(encoded_df.head())\n",
    "else:\n",
    "    print(\"No suitable columns found for one-hot encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e2760",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive data processing pipeline...\n",
      "\n",
      "1. Applying custom scaling...\n",
      "Applied scaling to column: task_grade\n",
      "Applied scaling to column: final_grade\n",
      "\n",
      "2. Creating categorical columns...\n",
      "Created nominal categorical for field\n",
      "Created nominal categorical for language\n",
      "Created ordered categorical for task_grade: ['dostateczny', 'dobry', 'bardzo dobry']\n",
      "Created ordered categorical for final_grade: ['niedostateczny', 'mierny', 'dostateczny']\n",
      "\n",
      "3. Extracting numeric values from text...\n",
      "Extracted numbers from jury_score -> jury_score_numeric\n",
      "\n",
      "4. Creating one-hot encodings...\n",
      "Column 'field' identified for one-hot encoding\n",
      "  Unique values: ['drones', 'media', 'racing', 'robotics']\n",
      "Column 'language' identified for one-hot encoding\n",
      "  Unique values: ['cplusplus', 'java', 'python', 'swift']\n",
      "\n",
      "One-hot encoding for 'field':\n",
      "  Original shape: (12,)\n",
      "  Encoded shape: (12, 4)\n",
      "  New columns: ['field_drones', 'field_media', 'field_racing', 'field_robotics']\n",
      "\n",
      "One-hot encoding for 'language':\n",
      "  Original shape: (12,)\n",
      "  Encoded shape: (12, 4)\n",
      "  New columns: ['language_cplusplus', 'language_java', 'language_python', 'language_swift']\n",
      "\n",
      "==================================================\n",
      "PROCESSING PIPELINE SUMMARY\n",
      "==================================================\n",
      "ORIGINAL:\n",
      "  Shape: (12, 11)\n",
      "  Columns: ['full_name', 'field', 'language', 'code', 'task_1', 'task_2', 'task_3', 'tasks_avg', 'task_grade', 'jury_score', 'final_grade']\n",
      "  Data types: {'full_name': dtype('O'), 'field': dtype('O'), 'language': dtype('O'), 'code': dtype('O'), 'task_1': dtype('float64'), 'task_2': dtype('float64'), 'task_3': dtype('float64'), 'tasks_avg': dtype('float64'), 'task_grade': dtype('O'), 'jury_score': dtype('O'), 'final_grade': dtype('O')}\n",
      "\n",
      "SCALED:\n",
      "  Shape: (12, 11)\n",
      "  Columns: ['full_name', 'field', 'language', 'code', 'task_1', 'task_2', 'task_3', 'tasks_avg', 'task_grade', 'jury_score', 'final_grade']\n",
      "  Data types: {'full_name': dtype('O'), 'field': dtype('O'), 'language': dtype('O'), 'code': dtype('O'), 'task_1': dtype('float64'), 'task_2': dtype('float64'), 'task_3': dtype('float64'), 'tasks_avg': dtype('float64'), 'task_grade': dtype('int64'), 'jury_score': dtype('O'), 'final_grade': dtype('int64')}\n",
      "\n",
      "CATEGORICAL:\n",
      "  Shape: (12, 11)\n",
      "  Columns: ['full_name', 'field', 'language', 'code', 'task_1', 'task_2', 'task_3', 'tasks_avg', 'task_grade', 'jury_score', 'final_grade']\n",
      "  Data types: {'full_name': dtype('O'), 'field': CategoricalDtype(categories=['drones', 'media', 'racing', 'robotics'], ordered=False, categories_dtype=object), 'language': CategoricalDtype(categories=['cplusplus', 'java', 'python', 'swift'], ordered=False, categories_dtype=object), 'code': dtype('O'), 'task_1': dtype('float64'), 'task_2': dtype('float64'), 'task_3': dtype('float64'), 'tasks_avg': dtype('float64'), 'task_grade': CategoricalDtype(categories=['dostateczny', 'dobry', 'bardzo dobry'], ordered=True, categories_dtype=object), 'jury_score': dtype('O'), 'final_grade': CategoricalDtype(categories=['niedostateczny', 'mierny', 'dostateczny'], ordered=True, categories_dtype=object)}\n",
      "\n",
      "EXTRACTED_NUMERIC:\n",
      "  Shape: (12, 1)\n",
      "  Columns: ['jury_score_numeric']\n",
      "  Data types: {'jury_score_numeric': dtype('float64')}\n",
      "\n",
      "ONE_HOT:\n",
      "  field: (12, 4) - ['field_drones', 'field_media', 'field_racing', 'field_robotics']\n",
      "  language: (12, 4) - ['language_cplusplus', 'language_java', 'language_python', 'language_swift']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_data_processing(dataframe, scale_mapping):\n",
    "    \n",
    "    processing_results = {\n",
    "        'original': dataframe.copy()\n",
    "    }\n",
    "    \n",
    "    # Step 1: Apply custom scaling\n",
    "    print(\"1. Applying custom scaling...\")\n",
    "    scaled_df, _ = apply_custom_scaling(dataframe, scale_mapping)\n",
    "    processing_results['scaled'] = scaled_df\n",
    "    \n",
    "    # Step 2: Create categorical versions\n",
    "    print(\"\\n2. Creating categorical columns...\")\n",
    "    categorical_df, _ = create_categorical_columns(dataframe, scale_mapping)\n",
    "    processing_results['categorical'] = categorical_df\n",
    "    \n",
    "    # Step 3: Extract numeric values\n",
    "    print(\"\\n3. Extracting numeric values from text...\")\n",
    "    numeric_df, _ = convert_text_to_numeric(dataframe)\n",
    "    if not numeric_df.empty:\n",
    "        processing_results['extracted_numeric'] = numeric_df\n",
    "    \n",
    "    # Step 4: One-hot encoding\n",
    "    print(\"\\n4. Creating one-hot encodings...\")\n",
    "    encoding_candidates = identify_encoding_candidates(dataframe, scale_mapping)\n",
    "    if encoding_candidates:\n",
    "        encoded_dfs = create_one_hot_encodings(dataframe, encoding_candidates)\n",
    "        processing_results['one_hot'] = encoded_dfs\n",
    "    \n",
    "    return processing_results\n",
    "\n",
    "# Run comprehensive processing\n",
    "all_results = comprehensive_data_processing(df_original, custom_scale)\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for step_name, result in all_results.items():\n",
    "    if isinstance(result, pd.DataFrame):\n",
    "        print(f\"{step_name.upper()}:\")\n",
    "        print(f\"  Shape: {result.shape}\")\n",
    "        print(f\"  Columns: {list(result.columns)}\")\n",
    "        print(f\"  Data types: {dict(result.dtypes)}\")\n",
    "    elif isinstance(result, dict):\n",
    "        print(f\"{step_name.upper()}:\")\n",
    "        for sub_name, sub_df in result.items():\n",
    "            print(f\"  {sub_name}: {sub_df.shape} - {list(sub_df.columns)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de467f",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bebe200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY ASSESSMENT\n",
      "========================================\n",
      "\n",
      "ORIGINAL:\n",
      "  rows: 12\n",
      "  columns: 11\n",
      "  completeness_pct: 100.0\n",
      "  data_retention_pct: 100.0\n",
      "  memory_usage_mb: 0.01\n",
      "\n",
      "SCALED:\n",
      "  rows: 12\n",
      "  columns: 11\n",
      "  completeness_pct: 100.0\n",
      "  data_retention_pct: 100.0\n",
      "  memory_usage_mb: 0.0\n",
      "\n",
      "CATEGORICAL:\n",
      "  rows: 12\n",
      "  columns: 11\n",
      "  completeness_pct: 100.0\n",
      "  data_retention_pct: 100.0\n",
      "  memory_usage_mb: 0.0\n",
      "\n",
      "EXTRACTED_NUMERIC:\n",
      "  rows: 12\n",
      "  columns: 1\n",
      "  completeness_pct: 83.33\n",
      "  data_retention_pct: 100.0\n",
      "  memory_usage_mb: 0.0\n",
      "\n",
      "Quality assessment saved to output/quality_assessment.json\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(processing_results):\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    original_df = processing_results['original']\n",
    "    original_rows = len(original_df)\n",
    "    \n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for step_name, result in processing_results.items():\n",
    "        if isinstance(result, pd.DataFrame) and step_name != 'one_hot':\n",
    "            # Calculate quality metrics\n",
    "            completeness = (1 - result.isnull().mean().mean()) * 100\n",
    "            data_retention = (len(result) / original_rows) * 100\n",
    "            \n",
    "            quality_metrics = {\n",
    "                'rows': len(result),\n",
    "                'columns': len(result.columns),\n",
    "                'completeness_pct': round(completeness, 2),\n",
    "                'data_retention_pct': round(data_retention, 2),\n",
    "                'memory_usage_mb': round(result.memory_usage(deep=True).sum() / 1024**2, 2)\n",
    "            }\n",
    "            \n",
    "            quality_report[step_name] = quality_metrics\n",
    "            \n",
    "            print(f\"\\n{step_name.upper()}:\")\n",
    "            for metric, value in quality_metrics.items():\n",
    "                print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Assess data quality\n",
    "quality_assessment = assess_data_quality(all_results)\n",
    "\n",
    "# Save quality report\n",
    "import json\n",
    "with open(OUTPUT_PATH / 'quality_assessment.json', 'w') as f:\n",
    "    json.dump(quality_assessment, f, indent=2)\n",
    "\n",
    "print(\"\\nQuality assessment saved to output/quality_assessment.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
