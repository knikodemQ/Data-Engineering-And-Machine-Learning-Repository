{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40746036",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow and Keras\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores deep learning fundamentals using TensorFlow and Keras, featuring two comprehensive projects:\n",
    "1. **Fashion MNIST Classification**: Multi-class image classification using feedforward neural networks\n",
    "2. **California Housing Regression**: Regression analysis with various neural network architectures\n",
    "\n",
    "### Key Objectives:\n",
    "1. Understand neural network architecture design\n",
    "2. Implement classification and regression models\n",
    "3. Apply proper data preprocessing and normalization\n",
    "4. Use callbacks for training optimization\n",
    "5. Compare different model architectures\n",
    "6. Visualize training progress with TensorBoard\n",
    "7. Evaluate model performance and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167f5d3",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, InputLayer, Normalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\" Environment Check:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Test GPU availability\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU devices detected: {len(gpus)}\")\n",
    "    if len(gpus) > 0:\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu}\")\n",
    "    else:\n",
    "        print(\"â„¹ CPU-only mode (no GPU detected)\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU check error: {e}\")\n",
    "\n",
    "# Test basic TensorFlow operations\n",
    "print(f\"\\n Basic TensorFlow test:\")\n",
    "try:\n",
    "    # Create a simple tensor\n",
    "    x = tf.constant([1, 2, 3, 4])\n",
    "    y = tf.constant([2, 4, 6, 8])\n",
    "    z = tf.add(x, y)\n",
    "    print(f\"Tensor operation test: {x.numpy()} + {y.numpy()} = {z.numpy()}\")\n",
    "    print(\" TensorFlow operations working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\" TensorFlow test failed: {e}\")\n",
    "\n",
    "# Memory growth for GPU (if available)\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\" GPU memory growth configured\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU config warning: {e}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Ready for deep learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad84643",
   "metadata": {},
   "source": [
    "# Part 1: Fashion MNIST Classification\n",
    "\n",
    "## Overview\n",
    "Fashion MNIST is a dataset of clothing images, serving as a drop-in replacement for the classic MNIST digits dataset. It contains 70,000 grayscale images in 10 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec0c43",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Verify data shapes\n",
    "assert X_train.shape == (60000, 28, 28)\n",
    "assert X_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training labels: {y_train.shape}\")\n",
    "print(f\"Test labels: {y_test.shape}\")\n",
    "print(f\"Pixel value range: {X_train.min()} - {X_train.max()}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = [\"koszulka\", \"spodnie\", \"pulower\", \"sukienka\", \"kurtka\",\n",
    "              \"sandaÅ‚\", \"koszula\", \"but\", \"torba\", \"kozak\"]\n",
    "\n",
    "class_names_en = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                 \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"Class names (Polish):\", class_names)\n",
    "print(\"Class names (English):\", class_names_en)\n",
    "\n",
    "# Show class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(unique, counts, alpha=0.7)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Fashion MNIST - Class Distribution in Training Set')\n",
    "plt.xticks(unique, [f\"{i}\\n{class_names[i]}\" for i in unique], rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/fashion_mnist_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, class_idx in enumerate(range(10)):\n",
    "    # Find first occurrence of each class\n",
    "    sample_idx = np.where(y_train == class_idx)[0][0]\n",
    "    \n",
    "    axes[i].imshow(X_train[sample_idx], cmap='binary')\n",
    "    axes[i].set_title(f'Class {class_idx}: {class_names[class_idx]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/fashion_mnist_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Show specific example (as in original lab)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(X_train[2137], cmap=\"binary\")\n",
    "plt.title(f'Sample 2137: {class_names[y_train[2137]]}')\n",
    "plt.axis('off')\n",
    "plt.savefig('data/fashion_mnist_sample_2137.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample 2137 class: {class_names[y_train[2137]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643af6af",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf38661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X_train_norm = X_train / 255.0\n",
    "X_test_norm = X_test / 255.0\n",
    "\n",
    "print(f\"Original range: {X_train.min()} - {X_train.max()}\")\n",
    "print(f\"Normalized range: {X_train_norm.min()} - {X_train_norm.max()}\")\n",
    "\n",
    "# Show comparison of original vs normalized image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(X_train[2137], cmap='binary')\n",
    "axes[0].set_title('Original (0-255)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(X_train_norm[2137], cmap='binary')\n",
    "axes[1].set_title('Normalized (0-1)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/normalization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc80693",
   "metadata": {},
   "source": [
    "## 4. Build and Train Fashion MNIST Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7864578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "model_fashion = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_fashion.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model_fashion.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9661fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "try:\n",
    "    tf.keras.utils.plot_model(model_fashion, \"data/fashion_mnist_model.png\", \n",
    "                             show_shapes=True, show_layer_names=True, dpi=150)\n",
    "    print(\"Model diagram saved successfully!\")\n",
    "except:\n",
    "    print(\"Could not create model diagram (requires pydot and graphviz)\")\n",
    "\n",
    "# Create a text-based architecture visualization\n",
    "def visualize_architecture_text(model):\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"Layer {i+1}: {layer.name}\")\n",
    "        print(f\"  Type: {type(layer).__name__}\")\n",
    "        if hasattr(layer, 'units'):\n",
    "            print(f\"  Units: {layer.units}\")\n",
    "        if hasattr(layer, 'activation'):\n",
    "            print(f\"  Activation: {layer.activation.__name__}\")\n",
    "        print(f\"  Output shape: {layer.output_shape}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "visualize_architecture_text(model_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_fashion.compile(loss='sparse_categorical_crossentropy',\n",
    "                     optimizer='sgd',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Setup TensorBoard logging\n",
    "log_dir = os.path.join(\"data\", \"logs\", \"fashion_mnist\", time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"Run 'tensorboard --logdir=data/logs' to view training progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Fashion MNIST classification model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_fashion = model_fashion.fit(X_train_norm, y_train, \n",
    "                                   epochs=20,\n",
    "                                   validation_split=0.1,\n",
    "                                   callbacks=[tensorboard_cb],\n",
    "                                   verbose=1)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ecf9d",
   "metadata": {},
   "source": [
    "## 5. Evaluate Fashion MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da06f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_title(f'{title} - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_title(f'{title} - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_training_history(history_fashion, 'Fashion MNIST Classification')\n",
    "plt.savefig('data/fashion_mnist_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e480fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model_fashion.evaluate(X_test_norm, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = model_fashion.predict(X_test_norm, verbose=0)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Fashion MNIST - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/fashion_mnist_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on random test samples (as in original lab)\n",
    "def predict_random_sample(model, X_test, y_test, class_names):\n",
    "    image_index = np.random.randint(len(X_test))\n",
    "    image = np.array([X_test[image_index]])\n",
    "    confidences = model.predict(image, verbose=0)\n",
    "    confidence = np.max(confidences[0])\n",
    "    prediction = np.argmax(confidences[0])\n",
    "    \n",
    "    print(f\"Prediction: {class_names[prediction]}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(f\"Truth: {class_names[y_test[image_index]]}\")\n",
    "    print(f\"Correct: {'âœ“' if prediction == y_test[image_index] else 'âœ—'}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image[0], cmap=\"binary\")\n",
    "    plt.title(f'Test Image\\nTrue: {class_names[y_test[image_index]]}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(10), confidences[0])\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Prediction Probabilities')\n",
    "    plt.xticks(range(10), [f'{i}\\n{class_names[i][:6]}' for i in range(10)], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return image_index\n",
    "\n",
    "# Predict a few random samples\n",
    "for i in range(3):\n",
    "    print(f\"\\nRandom Prediction {i+1}:\")\n",
    "    idx = predict_random_sample(model_fashion, X_test_norm, y_test, class_names)\n",
    "    plt.savefig(f'data/fashion_prediction_{i+1}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_fashion.save(\"data/fashion_clf.keras\")\n",
    "print(\"Fashion MNIST model saved as 'data/fashion_clf.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55538a",
   "metadata": {},
   "source": [
    "# Part 2: California Housing Regression\n",
    "\n",
    "## Overview\n",
    "The California Housing dataset contains information about housing prices in California. We'll use neural networks to predict house values based on various features like location, house age, and population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1d0d9",
   "metadata": {},
   "source": [
    "## 6. Load and Explore California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "print(\"Dataset description:\")\n",
    "print(housing.DESCR[:500] + \"...\")\n",
    "\n",
    "print(f\"\\nFeatures: {housing.feature_names}\")\n",
    "print(f\"Number of samples: {housing.data.shape[0]}\")\n",
    "print(f\"Number of features: {housing.data.shape[1]}\")\n",
    "print(f\"Target range: ${housing.target.min():.2f} - ${housing.target.max():.2f} (hundreds of thousands)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier analysis\n",
    "df_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df_housing['target'] = housing.target\n",
    "\n",
    "print(\"Dataset info:\")\n",
    "print(df_housing.info())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_housing.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, column in enumerate(df_housing.columns):\n",
    "    axes[i].hist(df_housing[column], bins=50, alpha=0.7)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/housing_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_housing.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f')\n",
    "plt.title('California Housing - Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/housing_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae08a20",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (as in original lab)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_valid.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed\")\n",
    "print(f\"Training features - mean: {X_train_scaled.mean(axis=0)[:3]}...\")\n",
    "print(f\"Training features - std: {X_train_scaled.std(axis=0)[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868a001",
   "metadata": {},
   "source": [
    "## 8. Build and Compare Multiple Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: 50 neurons per layer (as in original lab)\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "    InputLayer(input_shape=X_train.shape[1:]),\n",
    "    Normalization(mean=scaler.mean_, variance=scaler.var_),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_1.compile(loss=\"mean_squared_error\", \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "print(\"Model 1 (50 neurons per layer):\")\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True, min_delta=0.01)\n",
    "\n",
    "# Train Model 1\n",
    "print(\"Training Model 1...\")\n",
    "log_dir_1 = os.path.join(\"data\", \"logs\", \"housing\", \"model_1\", time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "os.makedirs(log_dir_1, exist_ok=True)\n",
    "tensorboard_cb_1 = TensorBoard(log_dir=log_dir_1)\n",
    "\n",
    "history_1 = model_1.fit(X_train, y_train, \n",
    "                       epochs=100, \n",
    "                       validation_data=(X_valid, y_valid), \n",
    "                       callbacks=[early_stopping_cb, tensorboard_cb_1],\n",
    "                       verbose=1)\n",
    "\n",
    "model_1.save(\"data/reg_housing_1.keras\")\n",
    "print(\"Model 1 training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: 44 neurons per layer (as in original lab)\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "    InputLayer(input_shape=X_train.shape[1:]),\n",
    "    Normalization(mean=scaler.mean_, variance=scaler.var_),\n",
    "    Dense(44, activation=\"relu\"),\n",
    "    Dense(44, activation=\"relu\"),\n",
    "    Dense(44, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_2.compile(loss=\"mean_squared_error\", \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "print(\"Training Model 2...\")\n",
    "log_dir_2 = os.path.join(\"data\", \"logs\", \"housing\", \"model_2\", time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "os.makedirs(log_dir_2, exist_ok=True)\n",
    "tensorboard_cb_2 = TensorBoard(log_dir=log_dir_2)\n",
    "\n",
    "history_2 = model_2.fit(X_train, y_train, \n",
    "                       epochs=100, \n",
    "                       validation_data=(X_valid, y_valid), \n",
    "                       callbacks=[early_stopping_cb, tensorboard_cb_2],\n",
    "                       verbose=1)\n",
    "\n",
    "model_2.save(\"data/reg_housing_2.keras\")\n",
    "print(\"Model 2 training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeedb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: 21 neurons per layer (as in original lab)\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    InputLayer(input_shape=X_train.shape[1:]),\n",
    "    Normalization(mean=scaler.mean_, variance=scaler.var_),\n",
    "    Dense(21, activation=\"relu\"),\n",
    "    Dense(21, activation=\"relu\"),\n",
    "    Dense(21, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_3.compile(loss=\"mean_squared_error\", \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "print(\"Training Model 3...\")\n",
    "log_dir_3 = os.path.join(\"data\", \"logs\", \"housing\", \"model_3\", time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))\n",
    "os.makedirs(log_dir_3, exist_ok=True)\n",
    "tensorboard_cb_3 = TensorBoard(log_dir=log_dir_3)\n",
    "\n",
    "history_3 = model_3.fit(X_train, y_train, \n",
    "                       epochs=100, \n",
    "                       validation_data=(X_valid, y_valid), \n",
    "                       callbacks=[early_stopping_cb, tensorboard_cb_3],\n",
    "                       verbose=1)\n",
    "\n",
    "model_3.save(\"data/reg_housing_3.keras\")\n",
    "print(\"Model 3 training completed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5ef7e",
   "metadata": {},
   "source": [
    "## 9. Compare Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories for all models\n",
    "def plot_regression_histories(histories, model_names):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    # Plot RMSE\n",
    "    for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "        axes[0].plot(history.history['root_mean_squared_error'], \n",
    "                    label=f'{name} - Training', color=color, linestyle='-')\n",
    "        axes[0].plot(history.history['val_root_mean_squared_error'], \n",
    "                    label=f'{name} - Validation', color=color, linestyle='--')\n",
    "    \n",
    "    axes[0].set_title('Model Comparison - RMSE')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Loss\n",
    "    for i, (history, name, color) in enumerate(zip(histories, model_names, colors)):\n",
    "        axes[1].plot(history.history['loss'], \n",
    "                    label=f'{name} - Training', color=color, linestyle='-')\n",
    "        axes[1].plot(history.history['val_loss'], \n",
    "                    label=f'{name} - Validation', color=color, linestyle='--')\n",
    "    \n",
    "    axes[1].set_title('Model Comparison - Loss (MSE)')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MSE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "histories = [history_1, history_2, history_3]\n",
    "model_names = ['Model 1 (50 neurons)', 'Model 2 (44 neurons)', 'Model 3 (21 neurons)']\n",
    "\n",
    "fig = plot_regression_histories(histories, model_names)\n",
    "plt.savefig('data/housing_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "models = [model_1, model_2, model_3]\n",
    "model_names = ['Model 1 (50)', 'Model 2 (44)', 'Model 3 (21)']\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Test evaluation\n",
    "    test_loss, test_rmse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    # Additional metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Parameters': model.count_params(),\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Test MAE': mae,\n",
    "        'RÂ² Score': r2,\n",
    "        'Final Training RMSE': min(model.history.history['root_mean_squared_error']),\n",
    "        'Final Validation RMSE': min(model.history.history['val_root_mean_squared_error'])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values for the best model\n",
    "best_model_idx = results_df['Test RMSE'].idxmin()\n",
    "best_model = models[best_model_idx]\n",
    "best_model_name = model_names[best_model_idx]\n",
    "\n",
    "y_pred_best = best_model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.6)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "axes[0].set_title(f'{best_model_name} - Predictions vs Actual')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].scatter(y_pred_best, residuals, alpha=0.6)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title(f'{best_model_name} - Residuals Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/housing_best_model_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Test RMSE: {results_df.iloc[best_model_idx]['Test RMSE']:.4f}\")\n",
    "print(f\"RÂ² Score: {results_df.iloc[best_model_idx]['RÂ² Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4e460",
   "metadata": {},
   "source": [
    "## 10. Model Interpretation and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8674db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def calculate_feature_importance(model, X_test, y_test, feature_names):\n",
    "    # Custom scoring function for Keras model\n",
    "    def keras_mse_scorer(model, X, y):\n",
    "        y_pred = model.predict(X, verbose=0).flatten()\n",
    "        return -mean_squared_error(y, y_pred)  # negative because higher is better\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test, y_test, \n",
    "        scoring=keras_mse_scorer,\n",
    "        n_repeats=10, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return perm_importance\n",
    "\n",
    "perm_importance = calculate_feature_importance(best_model, X_test, y_test, housing.feature_names)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': housing.feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title(f'{best_model_name} - Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/housing_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(feature_importance_df.sort_values('importance', ascending=False).to_string(index=False, float_format='%.6f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54183b65",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions\n",
    "\n",
    "### Deep Learning Results Summary\n",
    "\n",
    "#### Fashion MNIST Classification:\n",
    "- **Architecture**: Feedforward neural network with 2 hidden layers (300, 100 neurons)\n",
    "- **Test Accuracy**: High accuracy on clothing classification task\n",
    "- **Training Time**: Efficient training with 20 epochs\n",
    "- **Key Insights**: Deep learning effectively learns features from raw pixel data\n",
    "\n",
    "#### California Housing Regression:\n",
    "- **Best Model**: {best_model_name} with lowest test RMSE\n",
    "- **Architecture Comparison**: Tested 3 different network sizes (50, 44, 21 neurons)\n",
    "- **Performance**: Achieved good predictive accuracy for house price prediction\n",
    "- **Key Features**: Location-based features (longitude, latitude) most important\n",
    "\n",
    "### Key Deep Learning Concepts Demonstrated:\n",
    "\n",
    "1. **Neural Network Architecture Design**\n",
    "   - Layer selection and sizing\n",
    "   - Activation function choice\n",
    "   - Input/output layer configuration\n",
    "\n",
    "2. **Training Best Practices**\n",
    "   - Data normalization and preprocessing\n",
    "   - Train/validation/test splits\n",
    "   - Early stopping to prevent overfitting\n",
    "   - TensorBoard for monitoring\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Classification metrics (accuracy, confusion matrix)\n",
    "   - Regression metrics (RMSE, MAE, RÂ²)\n",
    "   - Model comparison and selection\n",
    "\n",
    "4. **Practical Applications**\n",
    "   - Image classification for computer vision\n",
    "   - Regression for continuous value prediction\n",
    "   - Feature importance analysis\n",
    "\n",
    "### Future Improvements:\n",
    "- Experiment with different optimizers and learning rates\n",
    "- Add regularization techniques (dropout, L1/L2)\n",
    "- Try different activation functions\n",
    "- Implement convolutional layers for image data\n",
    "- Explore ensemble methods combining multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
